---
layout: post
title: 泛化, VC 维, 模型复杂度
subtitle: 
tags: MachineLearning
category: tech
---

总结一下 [Learning From Data](https://work.caltech.edu/lectures.html) 这门课 Lecture 5~7 所学.

## 泛化问题

泛化能力是机器学习的关键. 我们利用训练集数据来训练模型, 目的不是得到最小的训练误差, 而是希望让泛化误差尽量小, 这样模型在遇到新鲜数据时才能有比较好的表现, 这样的模型才是有用的.

一般用测试误差来估计泛化误差. 当测试集数据量足够大时, 这种估计就是很准确的.

能否做到泛化, 决定了机器学习的可行性. 统计学习理论对这个问题给出了一些解答. 你会发现, 泛化的可行性是存在于概率意义上的.

//TODO: 补充逻辑: E_in 容易处理. 我们希望 E_out 接近 E_in

## Hoeffding 不等式

我们来回顾一下大数定律: 样本数量越多, 随机变量的平均值就越接近期望值. 为了说明机器学习中泛化的可能性, 我们可以利用大数定律的一种特殊形式: Hoeffding 不等式.

$$ P[|\nu - \mu| > \epsilon] \leq 2e^{-2\epsilon^2N}, \ \forall \epsilon > 0 $$

其中 $\nu$ 是样本平均值, $\mu$ 是期望值. 放在我们这里, 样本平均值就是训练集误差 $E_{in}$, 期望值就是泛化误差期望 $E_{out}$. 那么上面的不等式可以解读为: 当训练集数据量 N 足够大时, 泛化误差与训练集误差有很大概率会非常接近.

听起来很美, 是不是泛化问题就这样解决了呢? 

可惜不然. 以上不等式只适用于事先确定的单一假设函数. 而在机器学习过程中, 最终的那个假设函数 g 是从一个比较大的集合里选出来的. 如果集合大小是 M, 对集合内不同的假设函数取 union bound, 那么不等式就变成了:

$$ P[|E_{out}(g) - E_{in}(g)| > \epsilon] \leq 2Me^{-2\epsilon^2N}, \ \forall \epsilon > 0 $$

多了一个 M, 结果就完全变味了. 要知道, 一般机器学习的假设空间都包含无穷多的假设函数, M 是无穷大, 因此不等式右边会很大, 作为一个概率的限值已经没有意义了.

## "有效"假设函数

幸运的是, 上面这个 M 取得很保守, 实际情况并没有这么坏. 一个假设空间包含的假设函数虽然可能是无穷大, 但"有效"数量往往是有限的. 这个有效性表现在假设函数作用于训练数据集时的效果. 当不同的假设函数在训练集上得到的分类结果相同时(比如, 对三个数据点, 分类结果都是"正正负"), 有效数量就只有 1, 这些假设函数对应同一个"有效假设函数" -- dichotomy.

(以下都是就二分类问题而言) 假设函数"有效"数量的上限是 $2^N$ , 达到这个上限时, 意味着假设函数集合 H 穷尽了 N 个数据点分类的所有可能性 (引入术语"shatter", 我们可以说 H shatter 了数据点).

## VC dimension

为了抽象出一个表达模型复杂度的量, V 同学和 C 同学一起提出了 VC dimension, 即: 一个假设函数集合 H 最多能 shatter 多少数据点.

以二维平面的感知机为例, 我们可以想象, 对三个不共线的点, 它总是给出所有的分类可能, 但对四个点就办不到了. 因此二维感知机的 VC 维是 3. 其实更一般地, d 维感知机的 VC 维等于 d+1. 

## 泛化误差上界

借助 dichotomy 和 VC dimension 的概念, 经过一系列推导, 可以把前面不等式右端的概率限值缩小一些. 结论是, 下式以大于 $1-\delta$ 的概率成立:

$$ E_{out} \leq E_{in} + \Omega(N,H,\delta) $$ 

其中, 
+ $\Omega(N,H,\delta) = \sqrt{\frac{8}{N} ln\frac{4m_H(2N)}{\delta}}$
+ $m_H(N)$ 是"增长函数", 表示任选 N 个点时, H 能得到的最大的 dichotomy 数量.

进一步, 可以用 VC dimension 表示 $m_H(N)$ 的上界: $m_H(N) \leq N^{d_{vc}} + 1$

因此, $\Omega(N,H,\delta) \leq \sqrt{\frac{8}{N} ln\frac{4((2N)^{d_{vc}} + 1)}{\delta}}$

最终我们得到了这样一个泛化误差上界, 随模型 VC 维增大而增大, 随数据量增大而减小. 模型越复杂时, $E_{in}$ 更容易优化到比较小, 但 $\Omega$ 项会增大. 因此最佳的机器学习策略需要二者之间达到一种平衡.

// 泛化误差上界 究竟是指 E_out, 还是 E_out - E_in?
