---
layout: post
title: N-gram 语言模型
subtitle: 
tags: NLP
category: tech
---

### 什么是语言模型

给定词表 $ V = \\{x_1, x_2, … \\} $，一个句子可以看做词的序列 $x_1 x_2 … x_n$，将句子出现的概率记为 $p(x_1, x_2, … x_n)$，这样一个联合概率分布（族）就是一个语言模型。

语言的词表是非常庞大的，比如汉语的词表在10万量级。于是上述联合分布就会有 $\|V\|^n$ 种情况。这样的模型大而稀疏，不便于计算。怎样得到一个更紧凑的语言模型呢，可以通过引入马尔科夫假设来解决。

### 马尔科夫假设

联合分布 $p(x_1, x_2, … x_n)$ 可以写成 n 项条件概率相乘的形式，这些条件概率里的条件会比较长，对每个词要考虑它前面的所有词，这在实际中意义不大。

马尔科夫假设是指，每个词出现的概率只跟它前面的少数几个词有关。比如，二阶马尔科夫假设只考虑前面两个词，相应的语言模型是三元模型。引入了马尔科夫假设的语言模型，也可以叫做马尔科夫模型。

有了马尔科夫模型，怎样生成句子？可按照以下三个步骤：
1. 初始化 i = 1, 以及 $ x_0 = x_1 = * $ （\*为句首标识符）
2. 根据条件概率分布生成下一个词，循环执行直到 3
3. 如果生成的词为句末标识 STOP，就得到了一个句子


### 如何断句

断句问题在英文中有些麻烦 (见[wikipedia][1]），原因之一是 `.` 的意义模糊，它除了当做句号以外，还可以是小数点或省略号，也有可能出现在缩略词、URL、邮箱里，等等。NLTK 里似乎提供了英文断句工具。相比之下，中文的断句要愉快得多，句号基本没有歧义。 


### 三元模型及其缺陷

三元模型在实际应用中比较重要。《数学之美》中介绍说，如果N从3再往上增加时，效果提升不显著，但资源耗费增加得很快；Google 的罗塞塔翻译系统和语音搜索系统使用的是四元模型，存储需要 500 台以上的服务器！（这是2014年数据，最新情况未知）

三元模型的参数可以表示为 `q(w|u,v)`。这里的模型参数是总体分布参数，需要依据语料样本进行参数估计。比较自然的估计是极大似然估计：`q(w|u,v) = c(u,v,w)/c(u,v)`，其中 c 表示样本中的出现次数。

语言模型从 n 元到三元，虽然已经大大减少了参数个数，但仍然是个天文数字。按《数学之美》里给出的数字：汉语词汇量大致20万，那么三元模型的参数约有 $8*10^{15}$ 个，而互联网上的中文内容加起来只有 $10^{13}$ 量级。因此，采用极大似然估计的三元模型会遇到严重的问题：大部分的 `q(w|u,v)` 都会是零，这样其实低估了这些三元组的概率，因为它们虽然未在语料中出现，但实际中仍是有可能出现的。这时就需要对模型进行平滑化处理。


### 语言模型的平滑化

平滑化主要的原理大致可理解为，在必要的时候退化到低阶的模型，即一元、二元模型。常用的方法有两类，一是低阶与高阶的线性插值，二是对所有非零概率打折，匀出一小部分概率给未出现的词串。具体可见参考资料。

### 一个 N-gram 语言模型例子

见 [Github 目录](https://github.com/sunoonlee/deep-NLP-basics/tree/master/ngram). 确切地说，是 `Count based N-gram language model`.

#### 读取语料并进行预处理

选择《张爱玲作品集》作为语料输入，约 250 万字。经过初步考虑，我觉得可以对语料进行这样的预处理：
* 以 `。！？` 作为断句符号。为了方便生成句首词，在分词序列中这些断句符号之后，加入自定义的句首 token。
* 忽略部分类型的符号，包括`\n`、空格、引号、书名号等。
  * 之所以忽略引号、书名号、括号等成对符号，是担心生成语句时容易出现不配对的符号，看起来会很奇怪（后来试验的确如此）。
  * 忽略引号的另一个原因是，右引号常常放在上一条提到的断句符号之后，这种情况下，右引号实际上成为句末，处理这种情况会增加复杂性。

#### 统计 ngram 出现次数及概率

统计次数的任务可看做是上周任务的扩展。一开始我想的是用 `Counter`，以 `(context, word)` 元组为 key；后来看到视频演示中童牧老师用了 `defaultdict(Counter)`，想想确实更好用呀。

统计完出现次数后，可以将次数除以该 context 的总次数，得到估计的条件概率 `P(word|context)`。这样就得到了一个 `count based N-gram language model` 。

#### 试着生成几个句子

生成单个词的方法是参考了 demo 代码，具体是用一个累计的概率去跟随机数 r 比较，直到这个概率大于 r 为止。

生成句子的时候，每个句子需要预先设好最初的 context，好让程序能生成句首词，然后逐步平移 context 即可。

#### 生成效果随 N 的变化

N 取 2~5 时生成的句子样例分别见 `generated_Ngram.txt` (N = 2,3,4,5)

可以看出，N=2 时句子基本是混乱的， N 变大时句子更为通顺，但 N>3 时，直接照搬来的原句也较多。这种趋势大致可概括为：从「火星文」到「鹦鹉学舌」。应该是语料规模太小，不足以支撑起一个具有泛化能力的语言模型。


参考：

1. Michael Collins, [Course notes for NLP, Chapter 1: language modelling][2]
2. 吴军,《数学之美》第3章：统计语言模型

[1]:	https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation
[2]:	http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf
